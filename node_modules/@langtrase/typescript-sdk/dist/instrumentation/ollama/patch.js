"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.embeddingsPatch = exports.chatStreamPatch = exports.chatPatchNonStreamed = exports.generatePatchNonStreamed = exports.generateStreamPatch = exports.generatePatch = exports.chatPatch = void 0;
/* eslint-disable no-console */
const api_1 = require("@opentelemetry/api");
const common_1 = require("../../constants/common");
const trace_attributes_1 = require("@langtrase/trace-attributes");
const misc_1 = require("../../utils/misc");
const chatPatch = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (chatRequest) {
        if (chatRequest.stream === true) {
            return await (0, exports.chatStreamPatch)(original, tracer, langtraceVersion, sdkName, moduleVersion).apply(this, [chatRequest]);
        }
        return await (0, exports.chatPatchNonStreamed)(original, tracer, langtraceVersion, sdkName, moduleVersion).apply(this, [chatRequest]);
    };
};
exports.chatPatch = chatPatch;
const generatePatch = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (chatRequest) {
        if (chatRequest.stream === true) {
            return await (0, exports.generateStreamPatch)(original, tracer, langtraceVersion, sdkName, moduleVersion).apply(this, [chatRequest]);
        }
        return await (0, exports.generatePatchNonStreamed)(original, tracer, langtraceVersion, sdkName, moduleVersion).apply(this, [chatRequest]);
    };
};
exports.generatePatch = generatePatch;
const generateStreamPatch = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (generateRequest) {
        const customAttributes = api_1.context.active().getValue(common_1.LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY) ?? {};
        const prompts = [];
        if (generateRequest.system !== undefined) {
            prompts.push({ role: 'system', content: generateRequest.system });
        }
        prompts.push({ role: 'user', content: generateRequest.prompt });
        const attributes = {
            'langtrace.sdk.name': sdkName,
            'langtrace.service.name': 'ollama',
            'langtrace.service.type': 'llm',
            'langtrace.service.version': moduleVersion,
            'langtrace.version': langtraceVersion,
            'url.full': this?.config?.host,
            'url.path': trace_attributes_1.APIS.ollama.GENERATE.ENDPOINT,
            'gen_ai.request.model': generateRequest.model,
            'gen_ai.request.stream': generateRequest.stream,
            'gen_ai.request.temperature': generateRequest.options?.temperature,
            'gen_ai.request.top_p': generateRequest.options?.top_p,
            'http.timeout': Number.isNaN(Number(generateRequest?.keep_alive)) ? undefined : Number(generateRequest?.keep_alive),
            'gen_ai.request.frequency_penalty': generateRequest?.options?.frequency_penalty,
            'gen_ai.request.presence_penalty': generateRequest?.options?.presence_penalty,
            'gen_ai.request.response_format': generateRequest.format,
            ...customAttributes
        };
        const span = tracer.startSpan(trace_attributes_1.APIS.ollama.GENERATE.METHOD, { attributes, kind: api_1.SpanKind.CLIENT }, api_1.context.active());
        span.addEvent(trace_attributes_1.Event.GEN_AI_PROMPT, { 'gen_ai.prompt': JSON.stringify(prompts) });
        return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
            const resp = await original.apply(this, [generateRequest]);
            return (0, misc_1.createStreamProxy)(resp, handleGenerateStream(resp, attributes, span));
        });
    };
};
exports.generateStreamPatch = generateStreamPatch;
const generatePatchNonStreamed = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (generateRequest) {
        const customAttributes = api_1.context.active().getValue(common_1.LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY) ?? {};
        const prompts = [];
        if (generateRequest.system !== undefined) {
            prompts.push({ role: 'system', content: generateRequest.system });
        }
        prompts.push({ role: 'user', content: generateRequest.prompt });
        const attributes = {
            'langtrace.sdk.name': sdkName,
            'langtrace.service.name': 'ollama',
            'langtrace.service.type': 'llm',
            'langtrace.service.version': moduleVersion,
            'langtrace.version': langtraceVersion,
            'url.full': this?.config?.host,
            'url.path': trace_attributes_1.APIS.ollama.GENERATE.ENDPOINT,
            'gen_ai.request.model': generateRequest.model,
            'gen_ai.request.stream': generateRequest.stream,
            'gen_ai.request.temperature': generateRequest.options?.temperature,
            'gen_ai.request.top_p': generateRequest.options?.top_p,
            'http.timeout': Number.isNaN(Number(generateRequest?.keep_alive)) ? undefined : Number(generateRequest?.keep_alive),
            'gen_ai.request.frequency_penalty': generateRequest?.options?.frequency_penalty,
            'gen_ai.request.presence_penalty': generateRequest?.options?.presence_penalty,
            'gen_ai.request.response_format': generateRequest.format,
            ...customAttributes
        };
        const span = tracer.startSpan(trace_attributes_1.APIS.ollama.GENERATE.METHOD, { attributes, kind: api_1.SpanKind.CLIENT }, api_1.context.active());
        span.addEvent(trace_attributes_1.Event.GEN_AI_PROMPT, { 'gen_ai.prompt': JSON.stringify(prompts) });
        return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
            try {
                const resp = await original.apply(this, [generateRequest]);
                const responses = [{ content: resp.response, role: 'assistant' }];
                span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify(responses) });
                attributes['gen_ai.usage.prompt_tokens'] = resp?.prompt_eval_count;
                attributes['gen_ai.usage.completion_tokens'] = resp?.eval_count;
                attributes['gen_ai.usage.total_tokens'] = Number(resp?.prompt_eval_count ?? 0) + Number(resp?.eval_count ?? 0);
                attributes['gen_ai.response.model'] = resp.model;
                span.setAttributes(attributes);
                span.setStatus({ code: api_1.SpanStatusCode.OK });
                return resp;
            }
            finally {
                span.end();
            }
        });
    };
};
exports.generatePatchNonStreamed = generatePatchNonStreamed;
const chatPatchNonStreamed = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (chatRequest) {
        const customAttributes = api_1.context.active().getValue(common_1.LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY) ?? {};
        const prompts = chatRequest.messages?.map(({ role, content }) => ({ role: role.toLowerCase(), content })) ?? undefined;
        const attributes = {
            'langtrace.sdk.name': sdkName,
            'langtrace.service.name': 'ollama',
            'langtrace.service.type': 'llm',
            'langtrace.service.version': moduleVersion,
            'langtrace.version': langtraceVersion,
            'url.full': this?.config?.host,
            'url.path': trace_attributes_1.APIS.ollama.CHAT.ENDPOINT,
            'gen_ai.request.model': chatRequest.model,
            'gen_ai.request.stream': chatRequest.stream,
            'gen_ai.request.temperature': chatRequest.options?.temperature,
            'gen_ai.request.top_p': chatRequest.options?.top_p,
            'http.timeout': Number.isNaN(Number(chatRequest?.keep_alive)) ? undefined : Number(chatRequest?.keep_alive),
            'gen_ai.request.frequency_penalty': chatRequest?.options?.frequency_penalty,
            'gen_ai.request.presence_penalty': chatRequest?.options?.presence_penalty,
            'gen_ai.request.response_format': chatRequest.format,
            ...customAttributes
        };
        const span = tracer.startSpan(trace_attributes_1.APIS.ollama.CHAT.METHOD, { attributes, kind: api_1.SpanKind.CLIENT }, api_1.context.active());
        return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
            try {
                const resp = await original.apply(this, [chatRequest]);
                const responses = [{ content: resp.message.content, role: resp.message.role.toLowerCase() }];
                span.addEvent(trace_attributes_1.Event.GEN_AI_PROMPT, { 'gen_ai.prompt': JSON.stringify(prompts) });
                span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify(responses) });
                attributes['gen_ai.usage.prompt_tokens'] = resp?.prompt_eval_count;
                attributes['gen_ai.usage.completion_tokens'] = resp?.eval_count;
                attributes['gen_ai.usage.total_tokens'] = Number(resp?.prompt_eval_count ?? 0) + Number(resp?.eval_count ?? 0);
                attributes['gen_ai.response.model'] = resp.model;
                span.setAttributes(attributes);
                span.setStatus({ code: api_1.SpanStatusCode.OK });
                return resp;
            }
            finally {
                span.end();
            }
        });
    };
};
exports.chatPatchNonStreamed = chatPatchNonStreamed;
const chatStreamPatch = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (chatRequest) {
        const customAttributes = api_1.context.active().getValue(common_1.LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY) ?? {};
        const prompts = chatRequest.messages?.map(({ role, content }) => ({ role: role.toLowerCase(), content })) ?? undefined;
        const attributes = {
            'langtrace.sdk.name': sdkName,
            'langtrace.service.name': 'ollama',
            'langtrace.service.type': 'llm',
            'langtrace.service.version': moduleVersion,
            'langtrace.version': langtraceVersion,
            'url.full': this?.config.host,
            'url.path': trace_attributes_1.APIS.ollama.CHAT.ENDPOINT,
            'gen_ai.request.model': chatRequest.model,
            'gen_ai.request.stream': chatRequest.stream,
            'gen_ai.request.temperature': chatRequest.options?.temperature,
            'http.timeout': Number.isNaN(Number(chatRequest?.keep_alive)) ? undefined : Number(chatRequest?.keep_alive),
            'gen_ai.request.top_p': chatRequest.options?.top_p,
            'gen_ai.request.frequency_penalty': chatRequest?.options?.frequency_penalty,
            'gen_ai.request.presence_penalty': chatRequest?.options?.presence_penalty,
            'gen_ai.request.response_format': chatRequest.format,
            ...customAttributes
        };
        const span = tracer.startSpan(trace_attributes_1.APIS.ollama.CHAT.METHOD, { kind: api_1.SpanKind.CLIENT, attributes }, api_1.context.active());
        span.addEvent(trace_attributes_1.Event.GEN_AI_PROMPT, { 'gen_ai.prompt': JSON.stringify(prompts) });
        return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
            const resp = await original.apply(this, [chatRequest]);
            return (0, misc_1.createStreamProxy)(resp, handleChatStream(resp, attributes, span));
        });
    };
};
exports.chatStreamPatch = chatStreamPatch;
const embeddingsPatch = (original, tracer, langtraceVersion, sdkName, moduleVersion) => {
    return async function (request) {
        const customAttributes = api_1.context.active().getValue(common_1.LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY) ?? {};
        const attributes = {
            'langtrace.sdk.name': sdkName,
            'langtrace.service.name': 'ollama',
            'langtrace.service.type': 'llm',
            'langtrace.version': langtraceVersion,
            'langtrace.service.version': moduleVersion,
            'url.full': this.config.host,
            'url.path': trace_attributes_1.APIS.ollama.EMBEDDINGS.ENDPOINT,
            'gen_ai.request.model': request.model,
            'gen_ai.request.embedding_inputs': JSON.stringify(request.prompt),
            'http.timeout': Number.isNaN(Number(request.keep_alive)) ? undefined : Number(request.keep_alive),
            ...customAttributes
        };
        const span = tracer.startSpan(trace_attributes_1.APIS.ollama.EMBEDDINGS.METHOD, { kind: api_1.SpanKind.CLIENT, attributes }, api_1.context.active());
        try {
            return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
                const resp = await original.apply(this, [request]);
                span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify(resp) });
                span.setAttributes(attributes);
                span.setStatus({ code: api_1.SpanStatusCode.OK });
                return resp;
            });
        }
        catch (e) {
            span.setStatus({ code: api_1.SpanStatusCode.ERROR, message: e.message });
            throw e;
        }
        finally {
            span.end();
        }
    };
};
exports.embeddingsPatch = embeddingsPatch;
async function* handleChatStream(stream, attributes, span) {
    const responseReconstructed = [];
    try {
        span.addEvent(trace_attributes_1.Event.STREAM_START);
        for await (const chunk of stream) {
            span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION_CHUNK, { 'gen_ai.completion.chunk': JSON.stringify({ content: chunk.response ?? '', role: 'assistant' }) });
            responseReconstructed.push(chunk.message.content ?? '');
            if (chunk.done === true) {
                attributes['gen_ai.usage.prompt_tokens'] = chunk?.prompt_eval_count;
                attributes['gen_ai.usage.completion_tokens'] = chunk?.eval_count;
                attributes['gen_ai.usage.total_tokens'] = Number(chunk?.prompt_eval_count ?? 0) + Number(chunk?.eval_count ?? 0);
                attributes['gen_ai.response.model'] = chunk.model;
            }
            yield chunk;
        }
        span.addEvent(trace_attributes_1.Event.STREAM_END);
        span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify({ role: 'assistant', content: responseReconstructed.join('') }) });
        span.setAttributes(attributes);
        span.setStatus({ code: api_1.SpanStatusCode.OK });
    }
    catch (error) {
        span.setStatus({ code: api_1.SpanStatusCode.ERROR, message: error.message });
        throw error;
    }
    finally {
        span.end();
    }
}
async function* handleGenerateStream(stream, attributes, span) {
    const responseReconstructed = [];
    try {
        span.addEvent(trace_attributes_1.Event.STREAM_START);
        for await (const chunk of stream) {
            span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION_CHUNK, { 'gen_ai.completion.chunk': JSON.stringify({ content: chunk.response ?? '', role: 'assistant' }) });
            responseReconstructed.push(chunk.response ?? '');
            if (chunk.done === true) {
                attributes['gen_ai.usage.prompt_tokens'] = chunk?.prompt_eval_count;
                attributes['gen_ai.usage.completion_tokens'] = chunk?.eval_count;
                attributes['gen_ai.usage.total_tokens'] = Number(chunk?.prompt_eval_count ?? 0) + Number(chunk?.eval_count ?? 0);
                attributes['gen_ai.response.model'] = chunk.model;
            }
            yield chunk;
        }
        span.addEvent(trace_attributes_1.Event.STREAM_END);
        span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify({ role: 'assistant', content: responseReconstructed.join('') }) });
        span.setAttributes(attributes);
        span.setStatus({ code: api_1.SpanStatusCode.OK });
    }
    catch (error) {
        span.setStatus({ code: api_1.SpanStatusCode.ERROR, message: error.message });
        throw error;
    }
    finally {
        span.end();
    }
}
//# sourceMappingURL=patch.js.map