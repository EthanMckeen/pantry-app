"use strict";
/* eslint-disable @typescript-eslint/no-unsafe-return */
/*
 * Copyright (c) 2024 Scale3 Labs
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
Object.defineProperty(exports, "__esModule", { value: true });
exports.messagesCreate = void 0;
const common_1 = require("../../constants/common");
const misc_1 = require("../../utils/misc");
const trace_attributes_1 = require("@langtrase/trace-attributes");
const api_1 = require("@opentelemetry/api");
function messagesCreate(originalMethod, tracer, langtraceVersion, version) {
    return async function (...args) {
        // Determine the service provider
        const serviceProvider = trace_attributes_1.Vendors.ANTHROPIC;
        const customAttributes = api_1.context.active().getValue(common_1.LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY) ?? {};
        // Get the prompt and deep copy it
        let prompts = [];
        // Get the system message if any from args and attach it to the prompt with system role
        if (args[0]?.system !== undefined) {
            prompts.push({ role: 'system', content: args[0]?.system });
        }
        // Check if there are messages and concatenate them to the prompts array.
        if (args[0]?.messages !== undefined) {
            // eslint-disable-next-line @typescript-eslint/no-unsafe-argument
            prompts = prompts.concat(args[0].messages.map((msg) => ({ role: msg.role, content: msg.content })));
        }
        const attributes = {
            'langtrace.sdk.name': '@langtrase/typescript-sdk',
            'langtrace.service.name': serviceProvider,
            'langtrace.service.type': 'llm',
            'langtrace.service.version': version,
            'langtrace.version': langtraceVersion,
            'url.full': this?._client?.baseURL,
            'url.path': trace_attributes_1.APIS.anthropic.MESSAGES_CREATE.ENDPOINT,
            'gen_ai.request.model': args[0]?.model,
            'http.max.retries': this?._client?.maxRetries,
            'http.timeout': this?._client?.timeout,
            'gen_ai.request.temperature': args[0]?.temperature,
            'gen_ai.request.top_p': args[0]?.top_p,
            'gen_ai.request.top_k': args[0]?.top_k,
            'gen_ai.user': args[0]?.metadata?.user_id,
            'gen_ai.request.max_tokens': args[0]?.max_tokens,
            'gen_ai.response.model': args[0]?.model,
            ...customAttributes
        };
        if (!args[0].stream || args[0].stream === false) {
            const span = tracer.startSpan(trace_attributes_1.APIS.anthropic.MESSAGES_CREATE.METHOD, { kind: api_1.SpanKind.CLIENT, attributes }, api_1.context.active());
            return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
                try {
                    const resp = await originalMethod.apply(this, args);
                    span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify(resp.content.map((c) => ({ content: c.text, role: 'assistant' }))) });
                    span.addEvent(trace_attributes_1.Event.GEN_AI_PROMPT, { 'gen_ai.prompt': JSON.stringify(prompts) });
                    const respAttributes = {
                        'gen_ai.usage.completion_tokens': resp.usage.output_tokens,
                        'gen_ai.usage.prompt_tokens': resp.usage.input_tokens,
                        'gen_ai.usage.total_tokens': Number(resp.usage.output_tokens ?? 0) + Number(resp.usage.input_tokens ?? 0)
                    };
                    span.setAttributes({ ...attributes, ...respAttributes });
                    span.setStatus({ code: api_1.SpanStatusCode.OK });
                    return resp;
                }
                catch (error) {
                    span.setStatus({ code: api_1.SpanStatusCode.ERROR });
                    span.recordException(error);
                    throw error;
                }
                finally {
                    span.end();
                }
            });
        }
        else {
            const span = tracer.startSpan(trace_attributes_1.APIS.anthropic.MESSAGES_CREATE.METHOD, { kind: api_1.SpanKind.CLIENT, attributes }, api_1.context.active());
            return await api_1.context.with(api_1.trace.setSpan(api_1.context.active(), span), async () => {
                const resp = await originalMethod.apply(this, args);
                return (0, misc_1.createStreamProxy)(resp, handleStreamResponse(span, resp, attributes));
            });
        }
    };
}
exports.messagesCreate = messagesCreate;
async function* handleStreamResponse(span, stream, attributes) {
    const result = [];
    span.addEvent(trace_attributes_1.Event.STREAM_START);
    try {
        let streamStartMessage = {};
        for await (const chunk of stream) {
            if (chunk.type === 'message_start') {
                streamStartMessage = chunk.message;
            }
            else {
                const content = chunk.delta?.text !== undefined ? (chunk.delta.text).length > 0 ? chunk.delta.text : '' : '';
                const streamAttributes = { 'gen_ai.completion.chunk': JSON.stringify({ content, role: streamStartMessage.role }) };
                span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION_CHUNK, streamAttributes);
                result.push(content);
            }
            span.addEvent(trace_attributes_1.Event.GEN_AI_COMPLETION, { 'gen_ai.completion': JSON.stringify([{ content: result.join(''), role: streamStartMessage.role }]) });
            const responseAttributes = {
                'gen_ai.response.model': streamStartMessage.model,
                'gen_ai.usage.completion_tokens': streamStartMessage.usage.output_tokens,
                'gen_ai.usage.prompt_tokens': streamStartMessage.usage.input_tokens,
                'gen_ai.usage.total_tokens': Number(streamStartMessage.usage.output_tokens ?? 0) + Number(streamStartMessage.usage.input_tokens ?? 0)
            };
            span.setAttributes({ ...attributes, ...responseAttributes });
            yield chunk;
        }
        span.setStatus({ code: api_1.SpanStatusCode.OK });
        span.addEvent(trace_attributes_1.Event.STREAM_END);
    }
    catch (error) {
        span.setStatus({ code: api_1.SpanStatusCode.ERROR });
        span.recordException(error);
        throw error;
    }
    finally {
        span.end();
    }
}
//# sourceMappingURL=patch.js.map